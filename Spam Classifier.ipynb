{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.special import expit # Optimized sigmoid computation\n",
    "\n",
    "# Auto-reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.27299717  0.80694916  0.47809144  0.14567319  0.84806691  0.44178221\n",
      "  0.1602196   0.64611951  0.14825776  0.87767941  0.06802817  0.44049872\n",
      "  0.39804894  0.44671561  0.06451953  0.6772246   0.23810535  0.87558336\n",
      "  0.96555755  0.85068294  0.64611951  0.05543338  0.14816806  0.54643033\n",
      "  0.98532928  0.9122017   0.99508548  0.24644833  0.65089052  0.31863449\n",
      "  0.12843525  0.12843525  0.50980222  0.12843525  0.84876158  0.12843525\n",
      "  0.2062547   0.56920542  0.56361944  0.12843525  0.3799113   0.95858704\n",
      "  0.20755209  0.95152537  0.96786784  0.90383997  0.14777225  0.53896806\n",
      "  0.11105125  0.56970581  0.7760384   0.94534807  0.12564934  0.15661777\n",
      "  0.57719984  0.69710745  0.91043282]\n"
     ]
    }
   ],
   "source": [
    "# Get the data from the UCI archives\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
    "data = pd.read_csv(url, header=None)\n",
    "\n",
    "# Extract features and spam classification from dataframe\n",
    "X, y = data.iloc[:, :-1].values, data.iloc[:, -1].values\n",
    "\n",
    "# Save 20% for testing the finished model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Since train_test_split randomized, drop the last 10% of training data for cross_validation\n",
    "split = int(0.90 * X_train.shape[0])\n",
    "X_train, X_val = X_train[:split], X_train[split:]\n",
    "y_train, y_val = y_train[:split], y_train[split:]\n",
    "\n",
    "nm = Normalizer('l2')\n",
    "X_train_norm = nm.fit_transform(X_train)\n",
    "\n",
    "# Center and normalize the data\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "X_val = (X_val - mean) / std\n",
    "\n",
    "print(np.max(X_train_norm, axis=0) - np.min(X_train_norm, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "    \"\"\" \n",
    "    a naive implementation of numerical gradient of f at x \n",
    "    - f should be a function that takes a single argument\n",
    "    - x is the point (numpy array) to evaluate the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print(ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build our MLP class\n",
    "class mlp(object):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron class, implements a simple two layer neural network for binary classification\n",
    "    using ReLU activation for the hidden layer\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_hidden, std=1e-3):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        -------\n",
    "            n_features (int): number of features in input data\n",
    "            n_hidden (int): number of neurons (units) in hidden layer\n",
    "            std (float): multiplier for randomly initialized weight matrices\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.randn(n_features, n_hidden) * std\n",
    "        self.params['W2'] = np.random.randn(n_hidden, 1) * std\n",
    "        self.params['b1'] = np.zeros(n_hidden)\n",
    "        self.params['b2'] = np.zeros((1, 1))\n",
    "        \n",
    "    def loss(self, X, y=None, reg=0.01):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        -------\n",
    "        X (n-dimensional array): Input data\n",
    "        y (n-dimensional array): Output data (classifier labels)\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        If y is None, output the class scores\n",
    "        If y is given, output the loss and gradient WRT self.params\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = 0.0\n",
    "        grads = {}\n",
    "        for key in self.params:\n",
    "            # Initialize all gradient matrices to zero\n",
    "            grads[key] = np.zeros(self.params[key].shape)\n",
    "            \n",
    "        # Compute the feed-forward result of the network\n",
    "        z1 = np.dot(X, self.params['W1']) + self.params['b1'] # Samples x hidden count\n",
    "        a2 = np.maximum(0, z1) # Samples x hidden count\n",
    "        z3 = np.dot(a2, self.params['W2']) + self.params['b2'] # Samples x 1\n",
    "        a3 = expit(z3) # Samples x 1\n",
    "        scores = z3\n",
    "        \n",
    "        # If y was not passed, we are done\n",
    "        if y is None:\n",
    "            return scores\n",
    "        \n",
    "        loss = np.mean(y * np.log(a3) + (1 - y) * np.log(1 - a3))\n",
    "        loss -= 0.5 * reg * np.sum(self.params['W1'] * self.params['W1'])\n",
    "        loss -= 0.5 * reg * np.sum(self.params['W2'] * self.params['W2'])\n",
    "        \n",
    "        # Backpropagate the loss through the network\n",
    "        dscores = (y - a3) / X.shape[0] #/ X.shape[0] # Samples x 1\n",
    "        grads['W2'] = np.dot(a2.T, dscores)\n",
    "        grads['b2'] = np.sum(dscores, axis=0)\n",
    "        d_a2 = np.dot(dscores, self.params['W2'].T) # Samples x hidden count\n",
    "        d_z1 = d_a2 * (a2 > 0) # Samples x hidden count\n",
    "        grads['W1'] = np.dot(X.T, d_z1) # Features x hidden count\n",
    "        grads['b1'] = np.sum(d_z1, axis=0)\n",
    "        \n",
    "        grads['W2'] -= reg * self.params['W2']\n",
    "        grads['W1'] -= reg * self.params['W1']\n",
    "        \n",
    "        return loss, grads\n",
    "        \n",
    "    def predict(self, X):\n",
    "        scores = self.loss(X)\n",
    "        pred = scores > 0\n",
    "        return pred\n",
    "    \n",
    "    def train(self, X, y, learning_rate=1e-3, \n",
    "              nb_epoch=10, batch_size=512, reg=0.1,\n",
    "              learning_rate_decay=0.99):\n",
    "        X_batches, y_batches = [], []\n",
    "        for i in range(int(X.shape[0] / batch_size)):\n",
    "            X_batches.append(X[i * batch_size : (i+1) * batch_size, :])\n",
    "            y_batches.append(y[i * batch_size : (i+1) * batch_size])\n",
    "            \n",
    "        for epoch in range(nb_epoch):\n",
    "            for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "                loss, grads = self.loss(X_batch, y_batch, reg)\n",
    "                \n",
    "                # Adding the gradient might seem unusual, until we observe\n",
    "                # that the loss function takes the logarithm of a value less\n",
    "                # than zero, which always yields negative loss, meaning we\n",
    "                # are actually maximizing the loss during backpropagation\n",
    "                self.params['W1'] += learning_rate * grads['W1']\n",
    "                self.params['W2'] += learning_rate * grads['W2']\n",
    "                self.params['b1'] += learning_rate * grads['b1']\n",
    "                self.params['b2'] += learning_rate * grads['b2']\n",
    "            y_pred = self.predict(X_train)\n",
    "            acc = np.mean(y_train == y_pred)\n",
    "            print('\\rEpoch %d accuracy: %.2f' % (epoch, acc * 100), end='')\n",
    "            learning_rate *= learning_rate_decay\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 max relative error: 1.552781e-11\n",
      "b2 max relative error: 2.122749e-12\n",
      "W1 max relative error: 1.442160e-10\n",
      "b1 max relative error: 5.841179e-11\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(10, 4)\n",
    "b = np.random.randint(1, 10)\n",
    "\n",
    "net = mlp(a.shape[1], 5, np.sqrt(2 / a.shape[0]))\n",
    "loss, grads = net.loss(a, b, reg=0.1)\n",
    "\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(a, b, reg=0.1)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39930555555555558"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = mlp(X_train_norm.shape[1], 1000, std=np.sqrt(2 / X.shape[0]))\n",
    "np.mean(net.predict(X_train_norm) == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 accuracy: 60.51"
     ]
    }
   ],
   "source": [
    "net.train(X_train_norm, y_train[:, np.newaxis], nb_epoch=100, \n",
    "          learning_rate=5e-3, learning_rate_decay=0.999, reg=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
